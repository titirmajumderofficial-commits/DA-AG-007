{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "b0CH8tunAW8x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 1 : What is the difference between AI, ML, DL, and Data Science? Provide a brief explanation of each.\n",
        "\n",
        "Artificial Intelligence (AI) is the broadest field, representing the theory and development of computer systems capable of performing tasks that typically require human intelligence, such as visual perception, speech recognition, decision-making, and language translation..Its scope is to create intelligent agents that can reason, learn, and act autonomously.\n",
        "\n",
        "Machine Learning (ML) is a subset of AI.It is a technique that enables systems to automatically learn and improve from experience without being explicitly programmed.The core technique involves developing algorithms that can be trained on data to make predictions or decisions.\n",
        "\n",
        "Deep Learning (DL) is a specialized sub-field of ML.It employs neural networks with multiple hidden layers (hence, \"deep\") to model complex patterns in data.DL's techniques, such as Convolutional Neural Networks (CNNs) and Recurrent Neural Networks (RNNs), are primarily used in complex tasks like image recognition and natural language processing.\n",
        "\n",
        "Finally, Data Science is an interdisciplinary field that uses scientific methods, processes, algorithms, and systems to extract knowledge and insights from structured and unstructured data..Its scope is highly application-focused, blending statistics, computer science, and domain expertise to solve real-world problems, often utilizing ML and DL techniques as tools in the process.\n"
      ],
      "metadata": {
        "id": "P11MnFr4AZWZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Gqf7F35-Ddt7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 2: Explain overfitting and underfitting in ML. How can you detect and prevent them?\n",
        "\n",
        "Overfitting occurs when a model learns the training data too well, including the noise and random fluctuations, to the point where it performs poorly on new, unseen data..An overfit model has high variance and low bias.It is detected when the model shows very high accuracy on the training set but significantly lower accuracy on the validation or test set.\n",
        "\n",
        "Underfitting occurs when a model is too simple to capture the underlying structure or pattern in the training data, resulting in poor performance on both the training and test sets..An underfit model has high bias and high variance.It is detected when both the training and test accuracy scores are low.\n",
        "\n",
        "To prevent and detect these issues, one key technique is to manage the bias-variance tradeoff.Overfitting can be prevented by regularization techniques (like L1 or L2 regularization) which penalize complexity, by gathering more training data, or by simplifying the model..Underfitting is prevented by using a more complex model, introducing more relevant features, or reducing regularization.Cross-validation (eg, K-Fold) is a primary method for detection, as it allows for testing the model's performance on multiple subsets of the data, providing a more robust estimate of its generalization error."
      ],
      "metadata": {
        "id": "jKxQb-v3D_Rr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "FxUH51OKEXaB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 3: How would you handle missing values â€‹in a dataset? Explain at least three methods with examples.\n",
        "\n",
        "Handling missing values â€‹â€‹is a critical step in data preprocessing, and the chosen method depends on the extent and nature of the missingness..\n",
        "\n",
        "\n",
        "Deletion: This involves either Listwise Deletion (removing the entire row/sample that contains any missing value) or Pairwise Deletion (using only the available data for a specific analysis). For example, if a dataset has 5% of rows with missing 'Age' values, a data scientist might remove these 5% of rows (Listwise Deletion). This method is straightforward and used when the missing data is minimal and assumed to be Missing Completely at Random (MCAR).However, it risks losing valuable data and introducing bias if the missingness is not random.\n",
        "\n",
        "\n",
        "Imputation with Mean/Median: This technique involves replacing the missing value with the mean (for symmetrically distributed numerical data) or the median (for skewed numerical data) of the existing non-missing values â€‹â€‹in that feature..For instance, in a dataset of house prices where some 'Square Footage' entries are missing, one would calculate the median square footage of all known houses and use that value to fill the gaps..While simple, this method does not account for the variance and can lead to a less accurate model.\n",
        "\n",
        "\n",
        "Predictive Modeling (eg, Regression Imputation): This advanced technique treats the feature with missing values â€‹â€‹as the target variable and uses other features in the dataset as predictors to estimate the missing values.. For example, an analyst could use a Linear Regression model to predict a missing 'Salary' based on the 'Years of Experience' and 'Job Title' features, effectively using a sub-model to fill in the missing data.This method preserves the relationships between variables better than simple mean/median imputation but is more complex and time-consuming to implement."
      ],
      "metadata": {
        "id": "rcb_nKlyFQnR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 4: What is an imbalanced dataset? Describe two techniques to handle it\n",
        "(theoretical + practical).\n",
        "\n",
        "An unbalanced dataset is one where the distribution of the target variable's classes is not approximately equal, meaning one class (the majority class ) has significantly more observations than the other class(es) (the minority class )..This imbalance can cause a machine learning model to be biased towards the majority class, resulting in poor performance (eg, high accuracy but low recall for the minority class).\n",
        "\n",
        "Two techniques to handle unbalanced datasets are:\n",
        "\n",
        "Synthetic Minority Over-sampling Technique (SMOTE):\n",
        "\n",
        "\n",
        "Theoretical: SMOTE is an oversampling approach where synthetic samples are created for the minority class. It works by selecting a minority class sample and then choosing one or more of its k -nearest neighbors.A new synthetic instance is then created along the line segments joining the sample and its neighbors, rather than simply duplicating existing minority samples..\n",
        "\n",
        "Practical: Using the Python library imblearn, a data scientist would apply SMOTE()to the training data. This balances the class distribution, helping the model learn the characteristics of the minority class more effectively.\n",
        "\n",
        "Using Class Weights in Models:\n",
        "\n",
        "\n",
        "Theoretical: This technique does not alter the dataset itself but modifies the learning algorithm.By assigning higher weight to the minority class samples and lower weight to the majority class samples, the model is penalized more heavily for incorrect classifications of the minority class during training..This forces the model to pay more attention to the under-represented examples.\n",
        "\n",
        "Practical: Many ML algorithms, such as Logistic Regression and Support Vector Machines (SVMs) in scikit-learn, have a class_weightparameter that can be set to 'balanced'or manually specified.Setting it to 'balanced'automatically computes weights inversely proportional to class frequencies, directly addressing the imbalance during the training process."
      ],
      "metadata": {
        "id": "gtu6_cu4FjCZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 5: Why is feature scaling important in ML?\n",
        "Compare Min-Max scaling and Standardization.\n",
        "\n",
        "Feature scaling is an essential preprocessing step in machine learning that involves transforming the range of independent features to a standardized range..It is crucial because many ML algorithms rely on calculating the distance between data points or depend on the magnitude of feature values.Without scaling, features with larger initial magnitudes (eg, 'Salary' in the tens of thousands) would dominate the distance calculation over features with smaller magnitudes (eg, 'Years of Experience' from 1 to 20), leading to a model that is heavily biased towards the larger-scale features..This impact is particularly significant for distance-based algorithms like K-Nearest Neighbors (KNN) and Support Vector Machines (SVM)..Furthermore, for algorithms that rely on iterative optimization like Gradient Descent (used in Linear Regression and Neural Networks), unscaled features can cause the optimization path to oscillate wildly, resulting in slower convergence or failure to find the optimal solution.\n",
        "\n",
        "Comparison of Min-Max Scaling and Standardization\n",
        "Min-Max Scaling (Normalization) and Standardization (Z-Score Normalization) are two common techniques used in machine learning to scale features.\n",
        "\n",
        "Min-Max Scaling (Normalization)Min-Max Scaling transforms the data such that all feature values â€‹â€‹fall within a specific fixed range, typically [0, 1] . The formula used for this transformation is$X_{new} = \\frac{X - X_{min}}{X_{max} - X_{min}}$. This technique is generally preferred when the data distribution is not Gaussian or when clear boundaries, such as minimum and maximum values, are required, as is often the case in image processing. A significant drawback, however, is that Min-Max Scaling is highly sensitive to outliers . Since the transformation is based on the maximum and minimum values, outliers can drastically compress the range of the majority of the data points, reducing the effective distinction between non-outlier values.\n",
        "\n",
        "Standardization (Z-Score Normalization)Standardization rescales the data so that the resulting distribution has a mean ($\\mu$) of 0 and a standard deviation ($\\sigma$) of 1 . The formula for Standardization is$X_{new} = \\frac{X - \\mu}{\\sigma}$. This method is preferred when the feature distribution is approximately Gaussian (normal) or when the model requires normally distributed inputs, such as in Linear Discriminant Analysis. Unlike Min-Max Scaling, Standardization is less affected by outliers because the transformation does not constrain the data to a fixed, predefined range."
      ],
      "metadata": {
        "id": "egLK6O3MGMGY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 6: Compare Label Encoding and One-Hot Encoding.\n",
        "When would you prefer one over the other?\n",
        "\n",
        "Label Encoding and One-Hot Encoding are methods used to convert categorical variables into a numerical format that machine learning algorithms can process..\n",
        "\n",
        "\n",
        "Label Encoding assigns a unique integer to each category.For a feature like 'Color' with categories 'Red', 'Blue', and 'Green', it might be encoded as 0, 1, and 2, respectively..\n",
        "\n",
        "\n",
        "One-Hot Encoding creates a new binary feature (column) for each unique category in the original feature. In the 'Color' example, it would create three new columns: 'Color_Red', 'Color_Blue', and 'Color_Green'.For a data point that is 'Red', the 'Color_Red' column would be 1, and the others would be 0.\n",
        "\n",
        "Label Encoding Preference\n",
        "Label Encoding is preferred when dealing with Ordinal Variables . An ordinal variable is one where the categories have an inherent order or ranking (an ordinal relationship). A good example is 'Education Level,' where categories naturally progress (eg, High School < Bachelor's < Master's). By assigning sequential integers (eg, 0, 1, 2) to these categories, Label Encoding naturally captures this relationship, making the encoded variable useful for the machine learning model.\n",
        "\n",
        "One-Hot Encoding Preference\n",
        "One-Hot Encoding is preferred for Nominal Variables . A nominal variable is a categorical variable that has no intrinsic order (a nominal relationship), such as 'City' or 'Gender'. In this case, using Label Encoding would mistakenly introduce a false sense of ordinality (eg, implying 'City 2' is somehow 'greater' than 'City 1'), which can confuse and mislead the model. One-Hot Encoding avoids this problem by creating separate binary columns for each category, ensuring that all categories are treated as independent entities."
      ],
      "metadata": {
        "id": "P_6BlghNRV-M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "a). Analyze the relationship between app categories and ratings. Which categories have the highest/lowest average ratings, and what could be the possible reasons?\n",
        "\n",
        "Analytical Steps\n",
        "Here is the Python code structure and logic you need to execute the analysis:"
      ],
      "metadata": {
        "id": "fNhSvIQNR-zD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Initialize df to None\n",
        "df = None\n",
        "\n",
        "# 1. Data Loading (Assuming you have downloaded the CSV file, e.g., 'googleplaystore.csv')\n",
        "# You must first download the dataset from the provided GitHub link.\n",
        "try:\n",
        "    df = pd.read_csv('googleplaystore.csv')\n",
        "except FileNotFoundError:\n",
        "    print(\"Error: The CSV file was not found. Please download it from the specified GitHub repository.\")\n",
        "    # The program will not exit here, but will proceed with df = None\n",
        "    # Subsequent code will only run if df was successfully loaded.\n",
        "\n",
        "# Only proceed with data cleaning and analysis if the DataFrame was loaded successfully\n",
        "if df is not None:\n",
        "    # 2. Data Cleaning and Preparation\n",
        "    # The 'Rating' column is critical and contains missing values and potentially non-numeric entries.\n",
        "\n",
        "    # Drop rows where 'Rating' is missing, as imputation can bias category means.\n",
        "    df.dropna(subset=['Rating'], inplace=True)\n",
        "\n",
        "    # Ensure 'Rating' is numeric (it may already be, but good practice).\n",
        "    df['Rating'] = pd.to_numeric(df['Rating'], errors='coerce')\n",
        "    df.dropna(subset=['Rating'], inplace=True)\n",
        "\n",
        "    # 3. Analyze Relationship between Category and Average Rating\n",
        "    # Group the data by 'Category' and calculate the mean of the 'Rating'.\n",
        "    category_ratings = df.groupby('Category')['Rating'].mean().sort_values(ascending=False)\n",
        "\n",
        "    # 4. Identify Highest and Lowest Rated Categories\n",
        "    top_5_categories = category_ratings.head(5)\n",
        "    bottom_5_categories = category_ratings.tail(5)\n",
        "\n",
        "    # 5. Output the Results\n",
        "    print(\"## Categories with the Highest Average Ratings\")\n",
        "    print(top_5_categories.to_string())\n",
        "\n",
        "    print(\"\\n## Categories with the Lowest Average Ratings\")\n",
        "    print(bottom_5_categories.to_string())\n",
        "else:\n",
        "    print(\"Data loading failed. Please ensure 'googleplaystore.csv' is available.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z4_0yZ6nTVsl",
        "outputId": "ecc24116-f71a-4235-9814-7a28bcdc54f3"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error: The CSV file was not found. Please download it from the specified GitHub repository.\n",
            "Data loading failed. Please ensure 'googleplaystore.csv' is available.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "2vuzvrxCVF2v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 8: Titanic Dataset\n",
        "a) Compare the survival rates based on passenger class (Pclass). Which class had the highestsurvival rate, and why do you think that happened?\n",
        "\n",
        "The analysis of the Titanic dataset is a classic task in exploratory data analysis (EDA), revealing key demographic and social factors influencing survival. Since I cannot execute the Python code or access the external dataset, I will provide the detailed analytical steps (Python code and logic) you need and the expected findings and reasoning based on historical context.\n",
        "\n",
        "ðŸš¢ Analytical Steps (Python Code Outline)\n",
        "Here is the Python code structure and logic required to answer Question 8.\n"
      ],
      "metadata": {
        "id": "OHcEFfNDWAQ3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Initialize df to None\n",
        "df = None\n",
        "\n",
        "# 1. Data Loading from Google Drive\n",
        "try:\n",
        "    df = pd.read_csv('/content/drive/MyDrive/titanic.csv')\n",
        "except FileNotFoundError:\n",
        "    print(\"Error: The 'titanic.csv' file was not found in your Google Drive. Please ensure it's there and the path is correct.\")\n",
        "except Exception as e:\n",
        "    print(f\"An unexpected error occurred while loading the data: {e}\")\n",
        "\n",
        "# Only proceed with data cleaning and analysis if the DataFrame was loaded successfully\n",
        "if df is not None:\n",
        "    # 2. Data Cleaning and Preparation\n",
        "\n",
        "    # For Pclass analysis (Part a), Pclass and Survived are generally clean, but always check for missing values.\n",
        "    df.dropna(subset=['Pclass', 'Survived'], inplace=True)\n",
        "\n",
        "    # For Age analysis (Part b), handle missing 'Age' values. Imputation (like median) is common,\n",
        "    # but for simple survival rate comparison, dropping missing ages is often acceptable to keep groups pure.\n",
        "    age_df = df.dropna(subset=['Age', 'Survived']).copy()\n",
        "\n",
        "    ## Part a: Survival Rate by Pclass\n",
        "    print(\"--- Part a: Survival Rate by Passenger Class (Pclass) ---\")\n",
        "\n",
        "    # Group by Pclass and calculate the mean of the 'Survived' column (1=Survived, 0=Died)\n",
        "    # This mean represents the survival rate.\n",
        "    pclass_survival = df.groupby('Pclass')['Survived'].mean().sort_values(ascending=False)\n",
        "    pclass_survival = (pclass_survival * 100).round(2) # Convert to percentage\n",
        "\n",
        "    print(\"\\nPassenger Class Survival Rates:\")\n",
        "    print(pclass_survival.to_string() + ' %')\n",
        "\n",
        "    print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
        "\n",
        "    ## Part b: Survival Rate by Age Group\n",
        "    print(\"--- Part b: Survival Rate by Age Group (Child vs. Adult) ---\")\n",
        "\n",
        "    # Create the Age Group feature: Child (Age < 18) and Adult (Age >= 18)\n",
        "    age_df['Age_Group'] = np.where(age_df['Age'] < 18, 'Child', 'Adult')\n",
        "\n",
        "    # Group by the new 'Age_Group' and calculate the mean survival rate\n",
        "    age_survival = age_df.groupby('Age_Group')['Survived'].mean().sort_values(ascending=False)\n",
        "    age_survival = (age_survival * 100).round(2) # Convert to percentage\n",
        "\n",
        "    print(\"\\nAge Group Survival Rates:\")\n",
        "    print(age_survival.to_string() + ' %')\n",
        "else:\n",
        "    print(\"Data loading failed. Please ensure 'titanic.csv' is available and the path is correct.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "55zuLv64WP0Y",
        "outputId": "a71d50f2-3759-4b25-ce44-379414ab64e0"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error: The 'titanic.csv' file was not found in your Google Drive. Please ensure it's there and the path is correct.\n",
            "Data loading failed. Please ensure 'titanic.csv' is available and the path is correct.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b90bd54c",
        "outputId": "1d649dbe-16bc-4eb1-f702-d8d91d17f497"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dbf84d92"
      },
      "source": [
        "After executing the above cell, you'll be prompted to authorize Colab to access your Google Drive. Follow the instructions to complete the mounting process. Once mounted, you can place your `titanic.csv` file into your Google Drive (e.g., in 'My Drive') and then access it from Colab. For example, if it's directly in 'My Drive', you would load it like this:\n",
        "\n",
        "```python\n",
        "import pandas as pd\n",
        "df = pd.read_csv('/content/drive/MyDrive/titanic.csv')\n",
        "```\n",
        "\n",
        "If you place it in a subfolder like `Colab Notebooks`, it would be:\n",
        "\n",
        "```python\n",
        "import pandas as pd\n",
        "df = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/titanic.csv')\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rmj4owoiWxcR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 9: Flight Price Prediction Dataset\n",
        "a) How do flight prices vary with the days left until departure? Identify any exponential price\n",
        "surges and recommend the best booking window.\n",
        "b)Compare prices across airlines for the same route (e.g., Delhi-Mumbai). Which airlines are\n",
        "consistently cheaper/premium, and why?\n",
        "\n"
      ],
      "metadata": {
        "id": "WBEIZPEWX1NZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Initialize df to None\n",
        "df = None\n",
        "\n",
        "# 1. Data Loading and Cleaning (Assuming the dataset is downloaded)\n",
        "try:\n",
        "    df = pd.read_csv('Flight_Price_Prediction.csv')\n",
        "except FileNotFoundError:\n",
        "    print(\"Error: The CSV file was not found. Please download it from the specified GitHub repository.\")\n",
        "    # Do not exit, allow subsequent code to handle df being None\n",
        "except Exception as e:\n",
        "    print(f\"An unexpected error occurred while loading the data: {e}\")\n",
        "\n",
        "# Only proceed with data cleaning and analysis if the DataFrame was loaded successfully\n",
        "if df is not None:\n",
        "    # Convert date columns to datetime objects\n",
        "    df['Date_of_Journey'] = pd.to_datetime(df['Date_of_Journey'], format='%d/%m/%Y', errors='coerce')\n",
        "    df['Date_of_Booking'] = pd.to_datetime(df['Date_of_Booking'], format='%d/%m/%Y', errors='coerce')\n",
        "    df.dropna(subset=['Date_of_Journey', 'Date_of_Booking', 'Price'], inplace=True)\n",
        "\n",
        "\n",
        "    ## Part a: Price Variation vs. Days Left Until Departure\n",
        "    print(\"--- Part a: Price Variation vs. Days Left Until Departure ---\")\n",
        "\n",
        "    # Calculate Days Left Until Departure\n",
        "    df['Days_Left'] = (df['Date_of_Journey'] - df['Date_of_Booking']).dt.days\n",
        "\n",
        "    # Group by Days_Left and calculate the median price (median is robust to outliers)\n",
        "    price_vs_days_left = df.groupby('Days_Left')['Price'].median().reset_index()\n",
        "\n",
        "    # Sort by days left and print the results for observation\n",
        "    print(\"\\nMedian Price by Days Left (Top 5 and Bottom 5):\")\n",
        "    print(price_vs_days_left.sort_values(by='Days_Left', ascending=False).head(5).to_string())\n",
        "    print(price_vs_days_left.sort_values(by='Days_Left', ascending=True).head(5).to_string())\n",
        "\n",
        "    # Typically, you would visualize this using:\n",
        "    # import seaborn as sns\n",
        "    # sns.lineplot(data=price_vs_days_left, x='Days_Left', y='Price')\n",
        "\n",
        "\n",
        "    ## Part b: Price Comparison Across Airlines for a Specific Route (Delhi-Mumbai)\n",
        "    print(\"\\n--- Part b: Price Comparison Across Airlines for Delhi-Mumbai Route ---\")\n",
        "\n",
        "    # Filter the dataset for the specific route\n",
        "    delhi_mumbai_df = df[(df['Source'] == 'Delhi') & (df['Destination'] == 'Mumbai')].copy()\n",
        "\n",
        "    # Group by 'Airline' and calculate the mean price\n",
        "    airline_price_comparison = delhi_mumbai_df.groupby('Airline')['Price'].mean().sort_values()\n",
        "\n",
        "    print(\"\\nAverage Price Comparison for Delhi-Mumbai Route:\")\n",
        "    print(airline_price_comparison.to_string())\n",
        "\n",
        "    # Typically, you would visualize this using:\n",
        "    # sns.barplot(x=airline_price_comparison.index, y=airline_price_comparison.values)\n",
        "else:\n",
        "    print(\"Data loading failed. Please ensure 'Flight_Price_Prediction.csv' is available.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_4WvSVROYfI3",
        "outputId": "43ae6c7a-d419-4dde-93b8-d1fc7048873c"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error: The CSV file was not found. Please download it from the specified GitHub repository.\n",
            "Data loading failed. Please ensure 'Flight_Price_Prediction.csv' is available.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 10: HR Analytics Dataset\n",
        "a). What factors most strongly correlate with employee attrition? Use visualizations to show key\n",
        "drivers (e.g., satisfaction, overtime, salary).\n",
        "b). Are employees with more projects more likely to leave?\n",
        "\n"
      ],
      "metadata": {
        "id": "UV2C3j34Yoxb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# 1. Data Loading (You must download the hr_analytics dataset first)\n",
        "try:\n",
        "    df = pd.read_csv('hr_analytics.csv')\n",
        "except FileNotFoundError:\n",
        "    print(\"Error: The CSV file was not found. Please ensure it is in your working directory.\")\n",
        "    exit()\n",
        "\n",
        "# Ensure 'Attrition' is numeric for correlation (if it's 'Yes'/'No', convert it)\n",
        "# Assuming 'Attrition' is 1 for Yes (Left) and 0 for No (Stayed)\n",
        "df['Attrition_Numeric'] = df['Attrition'].apply(lambda x: 1 if x == 'Yes' else 0)\n",
        "\n",
        "## Part a: Correlation Analysis (Strongest Drivers)\n",
        "print(\"--- Part a: Factors Correlating with Employee Attrition ---\")\n",
        "\n",
        "# Convert high-impact categorical features like 'Overtime' and 'Salary' into numerical format\n",
        "# Overtime: Convert 'Yes' to 1 and 'No' to 0 for correlation\n",
        "df['Overtime_Numeric'] = df['Overtime'].apply(lambda x: 1 if x == 'Yes' else 0)\n",
        "\n",
        "# Salary: If salary is tiered (e.g., 'Low', 'Medium', 'High'), use Label Encoding (e.g., 1, 2, 3)\n",
        "# (Adjust this section based on the actual 'Salary' column name and values in your dataset)\n",
        "# Example: df['Salary_Numeric'] = df['Salary'].map({'Low': 1, 'Medium': 2, 'High': 3})\n",
        "\n",
        "# Calculate the correlation matrix for key features\n",
        "key_features = ['Attrition_Numeric', 'Satisfaction', 'Overtime_Numeric', 'Salary_Numeric', 'Number_of_Projects']\n",
        "correlation_matrix = df[key_features].corr()\n",
        "\n",
        "# Extract correlation of Attrition with other factors\n",
        "attrition_corr = correlation_matrix['Attrition_Numeric'].sort_values(ascending=False).drop('Attrition_Numeric')\n",
        "\n",
        "print(\"\\nCorrelation of Key Factors with Attrition:\")\n",
        "print(attrition_corr.to_string())\n",
        "\n",
        "# Visualization for Categorical Driver (Overtime)\n",
        "plt.figure(figsize=(6, 4))\n",
        "sns.barplot(x='Overtime', y='Attrition_Numeric', data=df)\n",
        "plt.title('Attrition Rate by Overtime Status')\n",
        "plt.ylabel('Attrition Rate (Mean)')\n",
        "plt.show() #\n",
        "\n",
        "## Part b: Projects vs. Attrition\n",
        "print(\"\\n--- Part b: Projects vs. Attrition ---\")\n",
        "\n",
        "# Calculate the attrition rate for each number of projects\n",
        "projects_attrition = df.groupby('Number_of_Projects')['Attrition_Numeric'].mean().reset_index()\n",
        "\n",
        "# Visualization\n",
        "plt.figure(figsize=(8, 5))\n",
        "sns.barplot(x='Number_of_Projects', y='Attrition_Numeric', data=projects_attrition)\n",
        "plt.title('Attrition Rate by Number of Projects')\n",
        "plt.ylabel('Attrition Rate (Mean)')\n",
        "plt.xlabel('Number of Projects')\n",
        "plt.show() #\n",
        "\n",
        "print(\"\\nAttrition Rate by Number of Projects:\")\n",
        "print(projects_attrition.to_string())"
      ],
      "metadata": {
        "id": "pXFhfV83ZLBg"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}